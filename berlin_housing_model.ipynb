{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mam863/Hauspreise/blob/main/berlin_housing_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/mam863/Hauspreise/main/wohnungen_mit_bezirk_excel.csv\n"
      ],
      "metadata": {
        "id": "4acNNnK3oQQJ",
        "outputId": "c7ceb77a-4575-4fd2-ab35-919635d41387",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-06-17 14:39:46--  https://raw.githubusercontent.com/mam863/Hauspreise/main/wohnungen_mit_bezirk_excel.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2025-06-17 14:39:46 ERROR 404: Not Found.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "XX58NZZ1TDjl"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Berlin Housing Price Prediction Model\n",
        "Following CRISP-DM methodology\n",
        "\n",
        "This script implements a machine learning model to predict housing prices in Berlin\n",
        "based on various features like area, rooms, location, etc.\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "import xgboost as XGBRegressor\n",
        "import joblib\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# 1. Business Understanding\n",
        "\"\"\"\n",
        "The goal of this model is to predict housing prices in Berlin based on various features.\n",
        "This can help:\n",
        "- Homebuyers make informed decisions\n",
        "- Real estate agents price properties competitively\n",
        "- Investors identify undervalued properties\n",
        "- Urban planners understand housing market dynamics\n",
        "\"\"\"\n",
        "\n",
        "# 2. Data Understanding\n",
        "print(\"Loading and exploring the dataset...\")\n",
        "# Load the data\n",
        "data = pd.read_csv('/content/wohnungen_mit_bezirk_excel.csv', sep=';')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "6wBlj8FETZ0I",
        "outputId": "1a4205d2-89e4-48df-d491-0d7981467af9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading and exploring the dataset...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/wohnungen_mit_bezirk_excel.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-2712921999>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading and exploring the dataset...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Load the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/wohnungen_mit_bezirk_excel.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m';'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/wohnungen_mit_bezirk_excel.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display basic information\n",
        "print(\"\\nDataset Shape:\", data.shape)\n",
        "print(\"\\nData Types:\")\n",
        "print(data.dtypes)\n",
        "print(\"\\nSummary Statistics:\")\n",
        "print(data.describe())"
      ],
      "metadata": {
        "id": "MoNMZPdIUPdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values\n",
        "print(\"\\nMissing Values:\")\n",
        "print(data.isnull().sum())\n"
      ],
      "metadata": {
        "id": "mB25mXYKUUf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Explore the target variable (price)\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(data['price'], kde=True)\n",
        "plt.title('Distribution of Housing Prices')\n",
        "plt.xlabel('Price (€)')\n",
        "plt.savefig('price_distribution.png')"
      ],
      "metadata": {
        "id": "IUmpGeivUYzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Explore relationships between features and target\n",
        "plt.figure(figsize=(12, 10))\n",
        "correlation = data.select_dtypes(include=[np.number]).corr()\n",
        "mask = np.triu(correlation)\n",
        "sns.heatmap(correlation, annot=True, mask=mask, cmap='coolwarm')\n",
        "plt.title('Correlation Matrix')\n",
        "plt.savefig('correlation_matrix.png')"
      ],
      "metadata": {
        "id": "b7rDa04rUfTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Explore geographical distribution\n",
        "plt.figure(figsize=(12, 8))\n",
        "borough_prices = data.groupby('Borough')['price'].mean().sort_values(ascending=False)\n",
        "borough_prices.plot(kind='bar')\n",
        "plt.title('Average Housing Price by Borough')\n",
        "plt.ylabel('Average Price (€)')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.savefig('borough_prices.png')"
      ],
      "metadata": {
        "id": "yS-OpfOlUkg3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Relationship between area and price\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x='area', y='price', data=data, alpha=0.6)\n",
        "plt.title('Price vs. Area')\n",
        "plt.xlabel('Area (m²)')\n",
        "plt.ylabel('Price (€)')\n",
        "plt.savefig('price_vs_area.png')\n"
      ],
      "metadata": {
        "id": "q4tHQk-wUp6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Data Preparation\n",
        "print(\"\\nPreparing the data...\")\n",
        "\n",
        "# Handle missing values in 'heating' and 'energy' columns\n",
        "# Replace 'unbekannt' with NaN\n",
        "data['heating'].replace('unbekannt', np.nan, inplace=True)\n",
        "data['energy'].replace('unbekannt', np.nan, inplace=True)\n"
      ],
      "metadata": {
        "id": "F9X6AqG0UvA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to clean construction_year (some values might be incorrect)\n",
        "def clean_year(year):\n",
        "    if pd.isna(year):\n",
        "        return np.nan\n",
        "    if year < 1800 or year > 2025:  # Assuming no buildings older than 1800 or newer than 2025\n",
        "        return np.nan\n",
        "    return year\n",
        "\n",
        "data['construction_year'] = data['construction_year'].apply(clean_year)"
      ],
      "metadata": {
        "id": "XgiqHv16U7sC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create feature for building age\n",
        "current_year = 2023\n",
        "data['building_age'] = current_year - data['construction_year']\n",
        "\n",
        "# Feature engineering: price per square meter\n",
        "data['price_per_sqm'] = data['price'] / data['area']"
      ],
      "metadata": {
        "id": "avGeNoXTVDVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop outliers based on price\n",
        "Q1 = data['price'].quantile(0.01)\n",
        "Q3 = data['price'].quantile(0.99)\n",
        "IQR = Q3 - Q1\n",
        "data_cleaned = data[(data['price'] >= Q1 - 1.5 * IQR) & (data['price'] <= Q3 + 1.5 * IQR)]\n"
      ],
      "metadata": {
        "id": "fpwyhuvMVL53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split features and target\n",
        "X = data_cleaned.drop(['price', 'price_per_sqm'], axis=1)\n",
        "y = data_cleaned['price']"
      ],
      "metadata": {
        "id": "0KnI9TydVR1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define categorical and numerical features\n",
        "categorical_features = ['energy', 'heating', 'Borough', 'Neighborhood']\n",
        "numerical_features = ['area', 'rooms', 'construction_year', 'level', 'building_age']\n",
        "\n",
        "# Define preprocessing for numerical features\n",
        "numerical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n"
      ],
      "metadata": {
        "id": "oVCXyefZVVx3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define preprocessing for categorical features\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])"
      ],
      "metadata": {
        "id": "TLqIp53GVbsy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine preprocessing steps\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ])\n"
      ],
      "metadata": {
        "id": "pGXlahKxViRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "MO5riFjpVnrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Modeling\n",
        "print(\"\\nTraining models...\")\n",
        "\n",
        "# Define different models to try\n",
        "models = {\n",
        "    'Random Forest': RandomForestRegressor(random_state=42),\n",
        "    'Gradient Boosting': GradientBoostingRegressor(random_state=42),\n",
        "    'ElasticNet': ElasticNet(random_state=42),\n",
        "    'XGBoost': XGBRegressor.XGBRegressor(random_state=42)\n",
        "}\n"
      ],
      "metadata": {
        "id": "kltDKnkCVr8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create directory for model artifacts\n",
        "if not os.path.exists('model_results'):\n",
        "    os.makedirs('model_results')\n"
      ],
      "metadata": {
        "id": "C8tDDkDbVwMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and evaluate each model\n",
        "results = {}\n",
        "for name, model in models.items():\n",
        "    print(f\"\\nTraining {name}...\")\n",
        "\n",
        "    # Create pipeline with preprocessing and model\n",
        "    pipeline = Pipeline(steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('model', model)\n",
        "    ])\n",
        "\n",
        "    # Train the model\n",
        "    pipeline.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate on test set\n",
        "    y_pred = pipeline.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    results[name] = {\n",
        "        'RMSE': rmse,\n",
        "        'MAE': mae,\n",
        "        'R2': r2,\n",
        "        'Model': pipeline\n",
        "    }\n",
        "\n",
        "    print(f\"{name} Results:\")\n",
        "    print(f\"RMSE: {rmse:.2f}\")\n",
        "    print(f\"MAE: {mae:.2f}\")\n",
        "    print(f\"R2: {r2:.4f}\")\n",
        "\n",
        "    # Save the model\n",
        "    joblib.dump(pipeline, f'model_results/{name.replace(\" \", \"_\").lower()}_model.pkl')\n",
        "\n",
        "    # Visualize actual vs predicted prices\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.scatter(y_test, y_pred, alpha=0.5)\n",
        "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
        "    plt.xlabel('Actual Prices')\n",
        "    plt.ylabel('Predicted Prices')\n",
        "    plt.title(f'{name}: Actual vs Predicted Prices')\n",
        "    plt.savefig(f'model_results/{name.replace(\" \", \"_\").lower()}_predictions.png')\n"
      ],
      "metadata": {
        "id": "rO_R9wSNVz6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Evaluation\n",
        "print(\"\\nEvaluating models...\")\n",
        "\n",
        "# Find best model based on RMSE\n",
        "best_model_name = min(results, key=lambda x: results[x]['RMSE'])\n",
        "best_model = results[best_model_name]['Model']\n",
        "best_rmse = results[best_model_name]['RMSE']\n",
        "best_r2 = results[best_model_name]['R2']\n",
        "\n",
        "print(f\"\\nBest Model: {best_model_name}\")\n",
        "print(f\"RMSE: {best_rmse:.2f}\")\n",
        "print(f\"R2: {best_r2:.4f}\")"
      ],
      "metadata": {
        "id": "K7KCs9OYWPWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For Random Forest or similar tree-based models, get feature importance\n",
        "if best_model_name in ['Random Forest', 'Gradient Boosting', 'XGBoost']:\n",
        "    # Get feature names after one-hot encoding\n",
        "    preprocessor = best_model.named_steps['preprocessor']\n",
        "    model = best_model.named_steps['model']\n",
        "\n",
        "    # Get feature names after preprocessing\n",
        "    if hasattr(preprocessor, 'get_feature_names_out'):\n",
        "        feature_names = preprocessor.get_feature_names_out()\n",
        "    else:\n",
        "        # Fallback for sklearn < 1.0\n",
        "        ohe_features = preprocessor.transformers_[1][1].named_steps['onehot'].get_feature_names_out(categorical_features)\n",
        "        feature_names = np.concatenate([numerical_features, ohe_features])\n",
        "\n",
        "    # Get feature importances\n",
        "    if hasattr(model, 'feature_importances_'):\n",
        "        importances = model.feature_importances_\n",
        "        feature_importance = pd.DataFrame({\n",
        "            'Feature': feature_names,\n",
        "            'Importance': importances\n",
        "        }).sort_values('Importance', ascending=False)\n",
        "\n",
        "        # Plot top 20 features\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        sns.barplot(x='Importance', y='Feature', data=feature_importance.head(20))\n",
        "        plt.title(f'Top 20 Feature Importances - {best_model_name}')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('model_results/feature_importance.png')\n",
        "        print(\"\\nTop 10 Important Features:\")\n",
        "        print(feature_importance.head(10))\n"
      ],
      "metadata": {
        "id": "cL2yWEtKWQvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the best model\n",
        "joblib.dump(best_model, 'best_model.pkl')\n",
        "\n",
        "# Create a simple prediction function\n",
        "def predict_price(features_dict):\n",
        "    \"\"\"\n",
        "    Make a price prediction using the best model.\n",
        "\n",
        "    Args:\n",
        "        features_dict: Dictionary with feature values\n",
        "\n",
        "    Returns:\n",
        "        Predicted price\n",
        "    \"\"\"\n",
        "    # Convert input dictionary to DataFrame\n",
        "    input_df = pd.DataFrame([features_dict])\n",
        "\n",
        "    # Make prediction\n",
        "    prediction = best_model.predict(input_df)[0]\n",
        "    return prediction\n",
        "\n",
        "# Example of how to use the prediction function\n",
        "example_property = {\n",
        "    'area': 80.0,\n",
        "    'rooms': 3.0,\n",
        "    'construction_year': 2000.0,\n",
        "    'level': 2.0,\n",
        "    'energy': 'Gas',\n",
        "    'heating': 'Zentralheizung',\n",
        "    'Borough': 'Mitte',\n",
        "    'Neighborhood': 'Mitte',\n",
        "    'building_age':33\n",
        "}\n",
        "\n",
        "print(\"\\nExample Prediction:\")\n",
        "try:\n",
        "    example_price = predict_price(example_property)\n",
        "    print(f\"Predicted price for example property: €{example_price:.2f}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error making prediction: {e}\")\n",
        "\n",
        "print(\"\\nModel development complete following CRISP-DM methodology.\")"
      ],
      "metadata": {
        "id": "v_2HR_iIlGO7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}